{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Original raw data comes in CSV format, every year of data in a separate file. In this module, we construct schema that adheres to [Frictionless Data specificaitons](https://frictionlessdata.io/specs/), validate each table and erase problematic entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data\n",
    "\n",
    "Create symlinks from data location to \"./in\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Convert to UTF-8\n",
    "\n",
    "Original data come in unknown encoding. Make best guess and save in UTF-8.\n",
    "\n",
    "### Validate against schema\n",
    "\n",
    "[Schema spec](https://frictionlessdata.io/specs/table-schema/)\n",
    "\n",
    "That code is in a separate repo, and is possibly outdated. For now download validated and cleaned files from GCS.\n",
    "\n",
    "```\n",
    "mkdir -p out/valid\n",
    "gsutil -m cp -r gs://info-group-corr/* out/valid/\n",
    "```\n",
    "\n",
    "The last 2 years (2016 and 2017) are missing.\n",
    "\n",
    "The schema is a separate JSON file, and can be used to assign correct data types when importing into BigQuery, Stata, pandas or any other dtype aware format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts\n",
    "\n",
    "It is handy to have smaller files for testing. Multiple approaches can be used to extract subsets in longitudinal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 100k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    " \n",
    "def extract_100k():\n",
    "    \"\"\"Create extract with header and first 100k records.\"\"\"\n",
    "    dir_in = Path('./out/valid')\n",
    "    dir_out = Path('./out/extracts/100k')\n",
    "    for fn_in in dir_in.glob('*.csv'):\n",
    "        fn_out = dir_out / fn_in.name\n",
    "        with open(fn_out, 'w') as fout:\n",
    "            subprocess.run(['head', '-n', '100001', fn_in], stdout=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notest\n",
    "extract_100k()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
