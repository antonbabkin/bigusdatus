{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "\n",
    "> Original raw data comes in CSV format, every year of data in a separate file. In this module, we construct schema that adheres to [Frictionless Data specificaitons](https://frictionlessdata.io/specs/), validate each table and erase problematic entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data\n",
    "\n",
    "Create symlinks from data location to \"./in\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against schema\n",
    "\n",
    "That code is in a separate repo, and is possibly outdated. For now download validated and cleaned files from GCS.\n",
    "\n",
    "```\n",
    "mkdir -p out/valid\n",
    "gsutil -m cp -r gs://info-group-corr/* out/valid/\n",
    "```\n",
    "\n",
    "The last 2 years (2016 and 2017) are missing.\n",
    "\n",
    "Another problem is that original data come in unknown encoding. Validated files are saved in UTF-8, which also reduces their byte size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts\n",
    "\n",
    "It is handy to have smaller files for testing. Multiple approaches can be used to extract subsets in longitudinal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 100k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    " \n",
    "def extract_100k():\n",
    "    \"\"\"Create extract with header and first 100k records.\"\"\"\n",
    "    dir_in = Path('./out/valid')\n",
    "    dir_out = Path('./out/extracts/100k')\n",
    "    for fn_in in dir_in.glob('*.csv'):\n",
    "        fn_out = dir_out / fn_in.name\n",
    "        with open(fn_out, 'w') as fout:\n",
    "            subprocess.run(['head', '-n', '100001', fn_in], stdout=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notest\n",
    "extract_100k()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To report number of lines, is it faster to iterate through file in Python or use system `wc` utility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def wc_py(fpath):\n",
    "    \"Return number of lines in a text file, using Python I/O.\"\n",
    "    with open(fpath) as f:\n",
    "        line_count = 0\n",
    "        for _ in f:\n",
    "            line_count += 1\n",
    "    return line_count\n",
    "\n",
    "def wc_sys(fpath):\n",
    "    \"Return number of lines in a text file, using sytem 'wc' utility.\"\n",
    "    p = subprocess.run(['wc', '-l', fpath], capture_output=True, text=True)\n",
    "    return int(p.stdout.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fpath = './README.md'\n",
    "assert wc_py(fpath) == wc_sys(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 s, sys: 1.47 s, total: 10.5 s\n",
      "Wall time: 10.4 s\n",
      "CPU times: user 6.76 ms, sys: 0 ns, total: 6.76 ms\n",
      "Wall time: 1.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11169277"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notest\n",
    "fpath = './out/valid/2000.csv'\n",
    "%time wc_py(fpath)\n",
    "%time wc_sys(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is faster to use sytem `wc` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from hurry.filesize import size\n",
    "\n",
    "def lsdir(fdir):\n",
    "    \"\"\"Return list of strings like \"file_name file_size number_of_lines\" for all files in :fdir:.\"\"\"\n",
    "    fpaths = []\n",
    "    for fname in os.listdir(fdir):\n",
    "        fpath = os.path.join(fdir, fname)\n",
    "        if not os.path.isfile(fpath):\n",
    "            continue\n",
    "        fpaths.append(fpath)\n",
    "    \n",
    "    info = ['Name\\tLines\\tSize']\n",
    "    for fpath in sorted(fpaths):\n",
    "        wc = wc_sys(fpath)\n",
    "        sz = size(os.path.getsize(fpath))\n",
    "        info.append(f'{fpath}\\t{wc}\\t{sz}')\n",
    "    return info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
