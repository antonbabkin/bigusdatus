{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet\n",
    "\n",
    "> Binary storage formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet\n",
    "\n",
    "File format for data storage. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. \n",
    "\n",
    "> Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n",
    "[Website](https://parquet.apache.org/)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Apache_Parquet)\n",
    "\n",
    "[Parquet vs CSV: AWS processing costs](https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04)\n",
    "\n",
    "[Docs](https://parquet.apache.org/documentation/latest/)\n",
    "\n",
    "Features:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure\n",
    "\n",
    "Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastparquet\n",
    "\n",
    "Python implementation of the *Apache Parquet* format.\n",
    "Part of *dask* ecosystem, designed to work well with dask for parallel execution.\n",
    "\n",
    "Latest release: 0.3.3\n",
    "\n",
    "Not all parts of the Parquet-format have been implemented yet or tested. \n",
    "Not all output options will be compatible with every other Parquet framework, which each implement only a subset of the standard.\n",
    "Usage decisions: writing parquet files that are compatible with other parquet implementations, versus performance when writing data for reading back with fastparquet.\n",
    "\n",
    "[GitHub page](https://github.com/dask/fastparquet)\n",
    "\n",
    "[Docs](https://fastparquet.readthedocs.io/en/latest/)\n",
    "\n",
    "### Variations\n",
    "\n",
    "- uncompressed, gzip, snappy (install `python-snappy` from conda-forge separately)\n",
    "\n",
    "### Issues\n",
    "\n",
    "- As of 0.3.3, pandas extended dtypes like \"Int64\" are not supported, but there is [wip](https://github.com/dask/fastparquet/pull/483)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "import ig_format\n",
    "from ig_format import pandas as igpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './out/extracts/100k'\n",
    "schema_path = './out/schema.json'\n",
    "data_years = range(1997, 2000)\n",
    "\n",
    "dt = igpd.dtypes_from_schema(schema_path)\n",
    "df = pd.read_csv(data_dir + '/2000.csv', dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nullable ints to floats for fastparquet compatibility\n",
    "# could be part of schema creation\n",
    "for c in df:\n",
    "    if isinstance(df[c].dtype, pd.Int64Dtype):\n",
    "        df[c] = df[c].astype('float64')\n",
    "\n",
    "fp.write('./tmp/2000.parquet', df, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = fp.ParquetFile('./tmp/2000.parquet')\n",
    "dfp = pf.to_pandas(['company', 'abi', 'state', 'naics', 'employees'])\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Arrow\n",
    "\n",
    "> Apache Arrow is a cross-language development platform for **in-memory** data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication.\n",
    "\n",
    "[Website](https://arrow.apache.org/)\n",
    "[Docs](https://arrow.apache.org/docs/index.html)\n",
    "\n",
    "The Arrow Python bindings (also named “PyArrow”) have first-class integration with NumPy, pandas, and built-in Python objects. They are based on the C++ implementation of Arrow.\n",
    "\n",
    "[Data types, Schema and Table](https://arrow.apache.org/docs/python/data.html)\n",
    "\n",
    "[Read and write parquet](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "> fastparquet has a much smaller footprint, weighing in at a modest 180 kB compared to pyarrow‘s hulking 48 MB. It also has a much simpler and pandas centric read/write signature which can be nice for users that are more comfortable working with a DataFrame mindset. On the other hand,pyarrow offers greater coverage of the Parquet file format, more voluminous documentation, and more explicit column typing which turned out to be important later on. After much wavering we decided to go forward using pyarrow. [parquet business use case](https://medium.com/when-i-work-data/por-que-parquet-2a3ec42141c6)\n",
    "\n",
    "Arrow's purpose is to move data between components (pandas, parquet, CSV, Spark, R, ...) more efficiently. If we only use pandas, fastparquet might be sufficient. For now, Arrow does not support SAS or Stata.\n",
    "\n",
    "\n",
    "### Feather\n",
    "\n",
    "[docs](https://arrow.apache.org/docs/python/ipc.html#feather-format)\n",
    "\n",
    "Lightweight file storage format for dataframes understood by both pandas and R. Does not look very active, it is probably better to use parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion from SAS\n",
    "\n",
    "### pyreadstat\n",
    "\n",
    "https://github.com/Roche/pyreadstat\n",
    "\n",
    "- convert between SAS, Stata and SPSS formats <-> pandas dataframes\n",
    "- wrapped C library, faster than sas7bdat and pd.read_sas\n",
    "- available from conda-forge\n",
    "- read meta, read column subset, read row chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
