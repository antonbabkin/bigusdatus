{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet\n",
    "\n",
    "> Binary storage formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet\n",
    "\n",
    "File format for data storage. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. \n",
    "\n",
    "> Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n",
    "[Website](https://parquet.apache.org/)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Apache_Parquet)\n",
    "\n",
    "[Parquet vs CSV: AWS processing costs](https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04)\n",
    "\n",
    "[Docs](https://parquet.apache.org/documentation/latest/)\n",
    "\n",
    "Features:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure\n",
    "\n",
    "Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastparquet\n",
    "\n",
    "Python implementation of the *Apache Parquet* format.\n",
    "Part of *dask* ecosystem, designed to work well with dask for parallel execution.\n",
    "\n",
    "Latest release: 0.3.3\n",
    "\n",
    "Not all parts of the Parquet-format have been implemented yet or tested. \n",
    "Not all output options will be compatible with every other Parquet framework, which each implement only a subset of the standard.\n",
    "Usage decisions: writing parquet files that are compatible with other parquet implementations, versus performance when writing data for reading back with fastparquet.\n",
    "\n",
    "[GitHub page](https://github.com/dask/fastparquet)\n",
    "\n",
    "[Docs](https://fastparquet.readthedocs.io/en/latest/)\n",
    "\n",
    "### Variations\n",
    "\n",
    "- uncompressed, gzip, snappy (install `python-snappy` from conda-forge separately)\n",
    "\n",
    "### Issues\n",
    "\n",
    "- As of 0.3.3, pandas extended dtypes like \"Int64\" are not supported, but there is [wip](https://github.com/dask/fastparquet/pull/483)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "import ig_format\n",
    "from ig_format import pandas as igpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './out/extracts/100k'\n",
    "schema_path = './out/schema.json'\n",
    "data_years = range(1997, 2000)\n",
    "\n",
    "dt = igpd.dtypes_from_schema(schema_path)\n",
    "df = pd.read_csv(data_dir + '/2000.csv', dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nullable ints to floats for fastparquet compatibility\n",
    "# could be part of schema creation\n",
    "for c in df:\n",
    "    if isinstance(df[c].dtype, pd.Int64Dtype):\n",
    "        df[c] = df[c].astype('float64')\n",
    "\n",
    "fp.write('./tmp/2000.parquet', df, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = fp.ParquetFile('./tmp/2000.parquet')\n",
    "dfp = pf.to_pandas(['company', 'abi', 'state', 'naics', 'employees'])\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion from SAS\n",
    "\n",
    "### pyreadstat\n",
    "\n",
    "https://github.com/Roche/pyreadstat\n",
    "\n",
    "- wrapped C library, faster than sas7bdat and pd.read_sas\n",
    "- available from conda-forge\n",
    "- supports SAS, Stata and SPSS formats\n",
    "- read meta, read column subset, read row chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
