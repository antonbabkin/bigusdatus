{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types optimization\n",
    "\n",
    "> Convert columns to use more memory efficient dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas columns are internally stored as numpy arrays, and so [NumPy data types](https://numpy.org/doc/stable/user/basics.types.html) are used.\n",
    "\n",
    "**Boolean**\n",
    "\n",
    "`np.bool_` takes 1 byte per item, but can not hold missing values. Logical operations on columns return series of this dtype, unless some of the element-wise tests results in NA value, in which case result is of `object` dtype.\n",
    "\n",
    "\n",
    "**Integer**\n",
    "\n",
    "Limits and other details can be looked up with `numpy.iinfo()`.\n",
    "\n",
    "Storing value outside of limits creates overflow.\n",
    "\n",
    "|  dtype | size (bytes) |             min            |             max            |\n",
    "|:------:|:------------:|:--------------------------:|:--------------------------:|\n",
    "| uint8  |       1      | 0                          | 255                        |\n",
    "| uint16 |       2      | 0                          | 65,535                     |\n",
    "| uint32 |       4      | 0                          | 4,294,967,295              |\n",
    "| uint64 |       8      | 0                          | 18,446,744,073,709,551,615 |\n",
    "| int8   |       1      | -128                       | 127                        |\n",
    "| int16  |       2      | -32,768                    | 32,767                     |\n",
    "| int32  |       4      | -2,147,483,648             | 2,147,483,647              |\n",
    "| int64  |       8      | -9,223,372,036,854,775,808 | 9,223,372,036,854,775,807  |\n",
    "\n",
    "Integer dtypes provide wide range of options, but the biggest constraint is that in standard pandas these dtypes do not allow for missing values in them.\n",
    "\n",
    "**Floating point**\n",
    "\n",
    "Wikipedia: [float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format), [float32](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), [float64](https://en.wikipedia.org/wiki/Double-precision_floating-point_format).\n",
    "\n",
    "Limits and other details can be looked up with `numpy.finfo()`.\n",
    "\n",
    "Spacing between a number and it's adjacent neighbor (`numpy.spacing()`) increases with number absolute magnitude. Therefore care should be taken when storing large integers as floats.\n",
    "\n",
    "\n",
    "|  dtype  | size (bytes) |           max           |       max exact integer       |\n",
    "|:-------:|:------------:|:-----------------------:|:-----------------------------:|\n",
    "| float16 |       2      |       6.55040e+04       |         $2^{11}$ = 2,048         |\n",
    "| float32 |       4      |      3.4028235e+38      |       $2^{24}$ = 16,777,216      |\n",
    "| float64 |       8      | 1.7976931348623157e+308 | $2^{53}$ = 9,007,199,254,740,992 |\n",
    "\n",
    "Even though `float16` might have good use cases (notably booleans with missing data), it looks like it is not always fully supported.\n",
    "\n",
    "**String**\n",
    "\n",
    "Although there are fixed length Unicode string dtype in NumPy (e.g. `np.dtype('U3')`), pandas uses `np.object_`. This is an array of pointers (item size of 32 or 64 bits, depending on platform architecture) to memory locations where actual strings are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floats spacing increases with number magnitude\n",
    "dt = np.float16\n",
    "info = np.finfo(dt)\n",
    "for exp in range(-16, 17):\n",
    "    x = 2**exp\n",
    "    npx = dt(x)\n",
    "    print(exp, x, npx, np.spacing(npx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer precision limits on floats\n",
    "for dt in [np.float16, np.float32, np.float64]:\n",
    "    info = np.finfo(dt)\n",
    "    max_int = 2**(info.nmant + 1)\n",
    "    print(dt.__name__, info.nmant, max_int)\n",
    "    assert dt(max_int - 1) != dt(max_int)\n",
    "    assert dt(max_int) == dt(max_int + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic conversion\n",
    "\n",
    "Be mindful of possible overflow when performing operations with numerical series, as dtypes will not always automatically convert to higher types.\n",
    "\n",
    "Result of series aggregation is numpy scalar with certain numpy dtype. \n",
    "Summation of ints results in `int64`, regardless of input dtype.\n",
    "Summation of `float32` remains `float32`, so precision may be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going beyond float32 integer precision\n",
    "s = pd.Series([2**24] * 3, dtype='float32')\n",
    "assert ((s + 1) == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 127 is max int8, but s.sum() does not overflow, because result is stored in int64\n",
    "s = pd.Series([127, 127, 127], dtype='int8')\n",
    "ss = s.sum()\n",
    "print(ss.dtype, ss, 127 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2**24 is largest int that can be exactly represented by float32\n",
    "s = pd.Series([2**24] * 3, dtype='float32')\n",
    "# s.sum() is float32, but number is not exact integer\n",
    "ss = s.sum()\n",
    "print(ss.dtype, ss, 2**24 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical\n",
    "\n",
    "[User guide](https://pandas.pydata.org/docs/user_guide/categorical.html)\n",
    "\n",
    "[#](https://pandas.pydata.org/docs/user_guide/categorical.html#missing-data)\n",
    "> Missing values should not be included in the Categorical’s categories, only in the values. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical’s codes, missing values will always have a code of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_unique_str(n, l, alphabet=None):\n",
    "    \"\"\"Return list of `n` random unique strings of lenght `l`.\"\"\"\n",
    "    if alphabet is None:\n",
    "        alphabet = string.ascii_lowercase\n",
    "    assert len(alphabet) ** l >= n, f'Can not generate {n} unique strings of length {l} from alphabet of length {len(alphabet)}.'\n",
    "    str_set = set()\n",
    "    while len(str_set) < n:\n",
    "        str_set.add(''.join(random.choices(alphabet, k=l)))\n",
    "    return list(str_set)\n",
    "    \n",
    "\n",
    "def gen_mock_data(n_rows, num=None, str_=None, cat=None):\n",
    "    \"\"\"Return dataframe with random data.\n",
    "    \n",
    "    `num`: number of columns.\n",
    "    \n",
    "    `str_`: {'n': number of colums, 'len': string length, 'nuni': number of uniques}.\n",
    "    If `str_` is number, it is interpreted as number of columns with defaults for other options.\n",
    "    \n",
    "    \n",
    "    `cat`: {'n': number of colums, 'len': string length, 'nuni': number of uniques}.\n",
    "    If `cat` is number, it is interpreted as number of columns with defaults for other options.\n",
    "    \"\"\"\n",
    "    \n",
    "    def str_df(par, categorical):\n",
    "        if isinstance(par, int):\n",
    "            par = {\n",
    "                'n': par,\n",
    "                'len': 8,\n",
    "                'nuni': n_rows // 10\n",
    "            }\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(par['n']):\n",
    "            uniques = gen_unique_str(par['nuni'], par['len'])\n",
    "            if categorical: \n",
    "                df[f'cat{i}'] = pd.Categorical(random.choices(uniques, k=n_rows), uniques)\n",
    "            else:\n",
    "                df[f'str{i}'] = random.choices(uniques, k=n_rows)\n",
    "        return df\n",
    "    \n",
    "    dfs = [pd.DataFrame({'id': range(n_rows)})]\n",
    "    \n",
    "    if num is not None:\n",
    "        dfs.append(pd.DataFrame(np.random.rand(n_rows, num), columns=[f'num{i}' for i in range(num)]))\n",
    "    if str_ is not None:\n",
    "        dfs.append(str_df(str_, False))\n",
    "    if cat is not None:\n",
    "        dfs.append(str_df(cat, True))\n",
    "    return pd.concat(dfs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and groupby\n",
    "\n",
    "Selection by equality test is x220 faster with categoricals.\n",
    "\n",
    "Groupby aggregation is x27 faster with categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_mock_data(1_000_000, str_=1, cat=1)\n",
    "print('select str')\n",
    "needle = df['str0'][0]\n",
    "%timeit _ = (df['str0'] == needle)\n",
    "print('select cat')\n",
    "needle = df['cat0'].cat.categories[0]\n",
    "%timeit _ = (df['cat0'] == needle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_mock_data(1_000_000, num=1, str_=1, cat=1)\n",
    "print('groupby str')\n",
    "%timeit _ = df.groupby('str0')['num0'].sum()\n",
    "print('groupby cat')\n",
    "%timeit _ = df.groupby('cat0')['num0'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String methods\n",
    "\n",
    "[#](https://pandas.pydata.org/docs/user_guide/categorical.html#string-and-datetime-accessors)\n",
    "\n",
    "`.str` and `.dt` accessors work on categoricals if categories are of an appropriate type.\n",
    "\n",
    "> The work is done on the categories and then a new Series is constructed. This has some performance implication if you have a Series of type string, where lots of elements are repeated (i.e. the number of unique elements in the Series is a lot smaller than the length of the Series). In this case it can be faster to convert the original Series to one of type category and use .str.\\<method\\> or .dt.\\<property\\> on that.\n",
    "\n",
    "About x8 speedup in `startswith()` and `contains()`, but the gain naturally declines as the share of unique values increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_mock_data(1_000_000, str_=1, cat=1)\n",
    "\n",
    "print('str: startswith')\n",
    "%timeit _ = df.str0.str.startswith('a')\n",
    "print('cat: startswith')\n",
    "%timeit _ = df.cat0.str.startswith('a')\n",
    "\n",
    "print('str: contains non-regex')\n",
    "%timeit _ = df.str0.str.contains('a', regex=False)\n",
    "print('cat: contains non-regex')\n",
    "%timeit _ = df.cat0.str.contains('a', regex=False)\n",
    "\n",
    "print('str: contains regex')\n",
    "%timeit _ = df.str0.str.contains('[ab]', regex=True)\n",
    "print('cat: contains regex')\n",
    "%timeit _ = df.cat0.str.contains('[ab]', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 10_000_000\n",
    "df = gen_mock_data(n_rows, str_=dict(n=1, len=10, nuni=n_rows//2), cat=dict(n=1, len=10, nuni=n_rows//2))\n",
    "\n",
    "print('str: startswith')\n",
    "%timeit _ = df.str0.str.startswith('a')\n",
    "print('cat: startswith')\n",
    "%timeit _ = df.cat0.str.startswith('a')\n",
    "\n",
    "print('str: contains non-regex')\n",
    "%timeit _ = df.str0.str.contains('a', regex=False)\n",
    "print('cat: contains non-regex')\n",
    "%timeit _ = df.cat0.str.contains('a', regex=False)\n",
    "\n",
    "print('str: contains regex')\n",
    "%timeit _ = df.str0.str.contains('[ab]', regex=True)\n",
    "print('cat: contains regex')\n",
    "%timeit _ = df.cat0.str.contains('[ab]', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "[#](https://pandas.pydata.org/docs/user_guide/categorical.html#merging-concatenation)\n",
    "[#](https://pandas.pydata.org/docs/user_guide/merging.html#merge-dtypes)\n",
    "\n",
    "> By default, combining Series or DataFrames which contain the same categories results in category dtype, otherwise results will depend on the dtype of the underlying categories. **Merges that result in non-categorical dtypes will likely have higher memory usage.** Use .astype or union_categoricals to ensure category results.\n",
    "\n",
    "> The category dtypes must be exactly the same, meaning the same categories and the ordered attribute. Otherwise the result will coerce to the categories’ dtype.\n",
    "\n",
    "> Merging on category dtypes that are the same can be quite performant compared to object dtype merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gen_cat_data(n_rows, n_cats, cat_len, cat):\n",
    "    cat_gen = itertools.product(string.ascii_lowercase, repeat=cat_len)\n",
    "    cats = [''.join(next(cat_gen)) for _ in range(n_cats)]\n",
    "    assert len(cats) == len(set(cats))\n",
    "    df = pd.DataFrame({'key': random.choices(cats, k=n_rows),\n",
    "                       'val': np.random.rand(n_rows)})\n",
    "    if cat: df['key'] = pd.Categorical(df['key'], cats)\n",
    "    agg = df.groupby('key')['val'].sum().rename('sum').reset_index()    \n",
    "    return df, agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}\n",
    "for rows_order in range(4, 9):\n",
    "    nr = 10**rows_order\n",
    "    for nc in [10, 1000]:\n",
    "        for cl in [5, 200]:\n",
    "            for c in [False, True]:\n",
    "                t = timeit.Timer('df.merge(agg)',\n",
    "                                 f'df, agg = gen_cat_data({nr}, {nc}, {cl}, {c})',\n",
    "                                 globals=globals())\n",
    "                repeats, time = t.autorange()\n",
    "                times[(nr, nc, cl, c)] = time / repeats\n",
    "\n",
    "df = pd.Series(times).rename_axis(index=['n_rows', 'n_cats', 'cat_len', 'cats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of strings does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.unstack('cat_len')\n",
    "x.iloc[:, 1] / x.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categoricals improve performance with 100k+ rows, up to x2 speedup with 100M rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.unstack('cat_len').mean(1)\n",
    "x = x.unstack('cats')\n",
    "x.iloc[:, 1] / x.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of categories slows down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.unstack('cat_len').mean(1)\n",
    "x = x.unstack('n_cats')\n",
    "(x.iloc[:, 1] / x.iloc[:, 0]).unstack('cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`time / n_rows` declines when few categoricals are used. No clear pattern otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.unstack('cat_len').mean(1)\n",
    "x /= x.index.get_level_values('n_rows')\n",
    "x.unstack(['cats', 'n_cats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If dataframe becomes wide\n",
    "\n",
    "When many columns are to be merged on one or both sides, merge starts taking significantly more time, mainly because data are to be copied to a new object.\n",
    "\n",
    "Merge on cat keys becomes slower than on str keys with wide dataframes, although difference is small compared to overall merge time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on strings\n",
    "df, agg = gen_cat_data(10_000_000, 100, 10, False)\n",
    "print('merge few columns')\n",
    "%time _ = df.merge(agg)\n",
    "for i in range(100):\n",
    "    df[f'var{i}'] = np.random.rand(len(df))\n",
    "print('merge many columns')\n",
    "%time _ = df.merge(agg)\n",
    "for i in range(100):\n",
    "    agg[f'agg{i}'] = np.random.rand(len(agg))\n",
    "print('merge many-many columns')\n",
    "%time _ = df.merge(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on categoricals\n",
    "df, agg = gen_cat_data(10_000_000, 100, 10, True)\n",
    "print('merge few columns')\n",
    "%time _ = df.merge(agg)\n",
    "for i in range(100):\n",
    "    df[f'var{i}'] = np.random.rand(len(df))\n",
    "print('merge many columns')\n",
    "%time _ = df.merge(agg)\n",
    "for i in range(100):\n",
    "    agg[f'agg{i}'] = np.random.rand(len(agg))\n",
    "print('merge many-many columns')\n",
    "%time _ = df.merge(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container for boolean with NA\n",
    "\n",
    "This might be a better solution than using `float32` (or less supported `float16`). Each item will only occupy one byte, and NA-related methods will work as expected.\n",
    "\n",
    "`fillna()` will not accept values outside of preset categories, so need to `add_categories()` first.\n",
    "\n",
    "Categories can be `[0, 1]` or `[False, True]`, but the latter is not supported by fastparquet writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_mock_data(100_000, num=1)\n",
    "df['boo'] = (df.num0 > 0.8)\n",
    "df.loc[df.sample(frac=0.1).index, 'boo'] = np.nan\n",
    "print(df.boo.value_counts(dropna=False))\n",
    "df['boo_cat'] = df.boo.astype('category').cat.rename_categories({0: False, 1: True})\n",
    "print(df.boo_cat.value_counts(dropna=False))\n",
    "print(df.memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and time\n",
    "\n",
    "To be added later when use case arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet support\n",
    "\n",
    "Integer and float types are stored and converted automatically with exception of `float16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fastparquet as pq\n",
    "\n",
    "data = list(range(100))\n",
    "df = pd.DataFrame()\n",
    "for dt in ['uint8', 'uint16', 'uint32', 'uint64',\n",
    "           'int8', 'int16', 'int32', 'int64',\n",
    "           'float16', 'float32', 'float64']:\n",
    "    df[dt] = pd.Series(data, dtype=dt)\n",
    "\n",
    "dfpq_path = '/tmp/dataframe.pq'\n",
    "df.to_parquet(dfpq_path, 'fastparquet', None, False)\n",
    "\n",
    "dfpq = pd.read_parquet(dfpq_path, 'fastparquet')\n",
    "pd.concat([df.dtypes, dfpq.dtypes], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories can not be `[False, True]`. Maybe fastparquet bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.Series([False, False, True], dtype=pd.CategoricalDtype([False, True])).to_frame('col') # <-- this fails\n",
    "df = pd.Series([False, False, True], dtype=pd.CategoricalDtype([False, True, 2])).to_frame('col') # <-- this works\n",
    "df.to_parquet('/tmp/dataframe.pq', 'fastparquet', None, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension types\n",
    "\n",
    "Newer versions of pandas introduced [extension dtypes](https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extending-extension-types), although they are still in experimental stage as of pandas 1.1. These include support for nullable [integers](https://pandas.pydata.org/docs/user_guide/integer_na.html) and [strings](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.StringDtype.html).\n",
    "\n",
    "Test performance before using."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
