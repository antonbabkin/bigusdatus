{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage\n",
    "\n",
    "> Storage alternatives. Read, write and convert files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "from ig_format import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV\n",
    "\n",
    "Human readable, universally supported, row oriented storage format.\n",
    "\n",
    "## Issues\n",
    "- Newly introduced nullable string dtype (pd.StringDtype, \"string\") substantially increases read time compared to old \"object\" type.\n",
    "\n",
    "## Possible optimizations\n",
    "- Use smallest possible dtype (e.g. float32 instead of float64)\n",
    "- Use categoricals for string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dtypes_from_schema(schema_path, ext_dtypes=False):\n",
    "    \"\"\"Read JSON table schema and convert it to \n",
    "    dictionary (col_name -> col_dtype) that can be passed to pandas CSV reader.\n",
    "    \"\"\"\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    if ext_dtypes:\n",
    "        dt_int = 'Int64'\n",
    "        dt_str = 'string'\n",
    "    else:\n",
    "        dt_int = 'float64'\n",
    "        dt_str = 'str'\n",
    "    map_ = {\n",
    "        'integer': dt_int,\n",
    "        'number': 'float64',\n",
    "        'string': dt_str,\n",
    "        'year': dt_int\n",
    "    }\n",
    "    dtypes = {f['name']: map_[f['type']] for f in schema['fields']}\n",
    "    return dtypes\n",
    "\n",
    "def read_csv(year, ext_dtypes=False, full=False, gzip=False):\n",
    "    \"\"\"Read single year of data from CSV file into pandas dataframe.\n",
    "    \n",
    "    :par ext_dtypes: use pandas experimental extension types (nullable int and str).\n",
    "    :par full: read all records or just the first 100k.\n",
    "    \"\"\"\n",
    "    path = f'./data/csv/{year}.csv'\n",
    "    if gzip: path += '.gz'\n",
    "    dt = dtypes_from_schema(f'./data/csv/{year}_schema.json', ext_dtypes)\n",
    "    nr = None if full else 100000\n",
    "    return pd.read_csv(path, dtype=dt, nrows=nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension types make reading slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = read_csv(2000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = read_csv(2000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv.gz\n",
    "\n",
    "Pandas can read compressed CSV files.\n",
    "Compression reduces size on disk and may improve read time by speeding up disk I/O at the expense of CPU time.\n",
    "\n",
    "Compression takes about the same time with Python \"gzip\" module and system \"gzip\" utility.\n",
    "Compressed file size is the same.\n",
    "\n",
    "Compression level can be set between 1 (fastest, worst compression) and 9 (slowest, best compression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compression level 1: fastest\n",
    "with open('./data/csv/2000.csv', 'rb') as f_in:\n",
    "    with gzip.open('./data/csv/2000.csv.py1.gz', 'wb', 1) as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compression level 6: same system \"gzip\" utility\n",
    "with open('./data/csv/2000.csv', 'rb') as f_in:\n",
    "    with gzip.open('./data/csv/2000.csv.py6.gz', 'wb', 6) as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# default compression level 9: highest\n",
    "with open('./data/csv/2000.csv', 'rb') as f_in:\n",
    "    with gzip.open('./data/csv/2000.csv.py9.gz', 'wb', 9) as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compress_files(*glob_paths):\n",
    "    \"\"\"Compress given files with GZIP.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.size_on_disk('./data/csv/2000.csv*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = read_csv(2000, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./data/csv/2000.csv.py1.gz ./data/csv/2000.csv.gz\n",
    "%time df = read_csv(2000, full=True, gzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./data/csv/2000.csv.py6.gz ./data/csv/2000.csv.gz\n",
    "%time df = read_csv(2000, full=True, gzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./data/csv/2000.csv.py9.gz ./data/csv/2000.csv.gz\n",
    "%time df = read_csv(2000, full=True, gzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet\n",
    "\n",
    "## Apache Parquet\n",
    "\n",
    "[Website](https://parquet.apache.org/)\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Apache_Parquet)\n",
    "[Docs](https://parquet.apache.org/documentation/latest/)\n",
    "\n",
    "File format for data storage. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. \n",
    "\n",
    "> Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n",
    "Features:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure\n",
    "\n",
    "Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages.\n",
    "\n",
    "[Parquet vs CSV: AWS processing costs](https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04)\n",
    "\n",
    "\n",
    "## fastparquet\n",
    "\n",
    "[GitHub](https://github.com/dask/fastparquet)\n",
    "[Docs](https://fastparquet.readthedocs.io/en/latest/)\n",
    "\n",
    "Python implementation of the *Apache Parquet* format.\n",
    "Part of *dask* ecosystem, designed to work well with dask for parallel execution.\n",
    "\n",
    "Latest release: 0.3.3\n",
    "\n",
    "Not all parts of the Parquet-format have been implemented yet or tested. \n",
    "Not all output options will be compatible with every other Parquet framework, which each implement only a subset of the standard.\n",
    "Usage decisions: writing parquet files that are compatible with other parquet implementations, versus performance when writing data for reading back with fastparquet.\n",
    "\n",
    "Compression: uncompressed, gzip, snappy (install `python-snappy` from conda-forge separately)\n",
    "\n",
    "### Issues\n",
    "\n",
    "- As of 0.3.3, pandas extended dtypes like \"Int64\" are not supported, but there is [PR](https://github.com/dask/fastparquet/pull/483)\n",
    "\n",
    "\n",
    "## Apache Arrow\n",
    "\n",
    "> Apache Arrow is a cross-language development platform for **in-memory** data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication.\n",
    "\n",
    "[Website](https://arrow.apache.org/)\n",
    "[Docs](https://arrow.apache.org/docs/index.html)\n",
    "\n",
    "The Arrow Python bindings (also named “PyArrow”) have first-class integration with NumPy, pandas, and built-in Python objects. They are based on the C++ implementation of Arrow.\n",
    "\n",
    "[Data types, Schema and Table](https://arrow.apache.org/docs/python/data.html)\n",
    "\n",
    "[Read and write parquet](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "> fastparquet has a much smaller footprint, weighing in at a modest 180 kB compared to pyarrow‘s hulking 48 MB. It also has a much simpler and pandas centric read/write signature which can be nice for users that are more comfortable working with a DataFrame mindset. On the other hand,pyarrow offers greater coverage of the Parquet file format, more voluminous documentation, and more explicit column typing which turned out to be important later on. After much wavering we decided to go forward using pyarrow. [parquet business use case](https://medium.com/when-i-work-data/por-que-parquet-2a3ec42141c6)\n",
    "\n",
    "Arrow's purpose is to move data between components (pandas, parquet, CSV, Spark, R, ...) more efficiently. If we only use pandas, fastparquet might be sufficient. As of now, Arrow does not support SAS or Stata.\n",
    "\n",
    "\n",
    "### Feather\n",
    "\n",
    "[Docs](https://arrow.apache.org/docs/python/ipc.html#feather-format)\n",
    "\n",
    "Lightweight file storage format for dataframes understood by both pandas and R. Does not look very active, it is probably better to use parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert CSV to parquet\n",
    "for year in range(1997, 2016):\n",
    "    df = read_csv(year, full=True)\n",
    "    for compres, label in zip([None, 'gzip', 'snappy'], ['', '.gz', '.snap']):\n",
    "        fp.write(f'./data/parquet/{year}{label}.pq', df, compression=compres, write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(2000)\n",
    "fp.write('/tmp/df.snappy.pq', df, compression='snappy', write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -sh data/parquet/????.pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert CSV to parquet\n",
    "data_years = range(1997, 2016)\n",
    "for year in data_years:\n",
    "    df = read_csv(year)\n",
    "    fp.write(f'./data/parquet/{year}.gz.pq', df, compression='gzip', write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.size_on_disk('./data/parquet/????.gz.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert CSV to parquet\n",
    "data_years = range(1997, 2016)\n",
    "for year in data_years:\n",
    "    df = read_csv(year)\n",
    "    fp.write(f'./data/parquet/{year}.snappy.pq', df, compression='snappy', write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -sh data/parquet/????.snappy.pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pq(year, cols=None):\n",
    "    \"\"\"Read single year of data from CSV file into pandas dataframe.\n",
    "    \n",
    "    :par cols: list of columns to read (default all).\n",
    "    \"\"\"\n",
    "    pf = fp.ParquetFile(f'./data/parquet/{year}.pq')\n",
    "    return pf.to_pandas(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = read_csv(2013)\n",
    "dfp = read_pq(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dfc.dtypes == dfp.dtypes).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# omnischema\n",
    "\n",
    "This is an idea of a Python package that would provide conversion between different table schema formats.\n",
    "\n",
    "What I need now: conversion between pandas and parquet.\n",
    "\n",
    "Schemas that could be supported:\n",
    "\n",
    "- pandas\n",
    "  - classic [numpy dtypes](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html)\n",
    "  - new [extension types](https://pandas.pydata.org/docs/getting_started/basics.html#basics-dtypes)\n",
    "- [parquet](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md)\n",
    "- [Frictionless Data Table Schema](https://frictionlessdata.io/specs/table-schema/)\n",
    "- SQL (flavors)\n",
    "- Apache Arrow\n",
    "- R dataframe\n",
    "- Stata\n",
    "- SAS\n",
    "- BigQuery\n",
    "- xlsx, ods\n",
    "\n",
    "## Existing tools\n",
    "\n",
    "- [Stat/Transfer](https://stattransfer.com/) - commercial software for format conversion. It even support Feather.\n",
    "- [pyreadstat](https://github.com/Roche/pyreadstat)\n",
    "  - convert SAS, Stata and SPSS formats <-> pandas dataframes\n",
    "  - wrapped C library, faster than sas7bdat and pd.read_sas\n",
    "  - available from conda-forge\n",
    "  - read meta, read column subset, read row chunks\n",
    "- [odo](http://odo.pydata.org/en/latest/) - efficiently migrates data from the source to the target through a network of conversions.\n",
    "\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "- [DCAT](https://www.w3.org/TR/vocab-dcat/) - W3C spec for dataset metadata\n",
    "- [Schema.org Dataset](https://schema.org/Dataset) - founded by major search engines, used by Google Dataset Search tool.\n",
    "- [Data Documentation Initiative](https://ddialliance.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
