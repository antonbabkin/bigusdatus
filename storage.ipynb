{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage\n",
    "\n",
    "> Storage alternatives. Read, write and convert files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV\n",
    "\n",
    "## Issues\n",
    "- Newly introduced nullable string dtype (pd.StringDtype, \"string\") substantially increases read time compared to old \"object\" type. Need to profile this and choose an approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dtypes_from_schema(schema_path):\n",
    "    \"\"\"Read JSON table schema and convert it to \n",
    "    dictionary (col_name -> col_dtype) that can be passed to pandas CSV reader.\"\"\"\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    map_ = {\n",
    "        'integer': 'Int64',\n",
    "        'number': 'float64',\n",
    "        'string': 'string',\n",
    "        'year': 'Int64'\n",
    "    }\n",
    "    dtypes = {f['name']: map_[f['type']] for f in schema['fields']}\n",
    "    return dtypes\n",
    "\n",
    "def read_csv(year, full=False):\n",
    "    \"\"\"Read single year of data from CSV file into pandas dataframe.\n",
    "    \n",
    "    :par full: read all records or just the first 100k.\n",
    "    \"\"\"\n",
    "    path = f'data/csv/{year}.csv'\n",
    "    dt = dtypes_from_schema(f'data/csv/{year}_schema.json')\n",
    "    nr = None if full else 100000\n",
    "    return pd.read_csv(path, dtype=dt, nrows=nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet\n",
    "\n",
    "File format for data storage. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. \n",
    "\n",
    "> Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n",
    "[Website](https://parquet.apache.org/)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Apache_Parquet)\n",
    "\n",
    "[Parquet vs CSV: AWS processing costs](https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04)\n",
    "\n",
    "[Docs](https://parquet.apache.org/documentation/latest/)\n",
    "\n",
    "Features:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure\n",
    "\n",
    "Hierarchically, a file consists of one or more row groups. A row group contains exactly one column chunk per column. Column chunks contain one or more pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastparquet\n",
    "\n",
    "Python implementation of the *Apache Parquet* format.\n",
    "Part of *dask* ecosystem, designed to work well with dask for parallel execution.\n",
    "\n",
    "Latest release: 0.3.3\n",
    "\n",
    "Not all parts of the Parquet-format have been implemented yet or tested. \n",
    "Not all output options will be compatible with every other Parquet framework, which each implement only a subset of the standard.\n",
    "Usage decisions: writing parquet files that are compatible with other parquet implementations, versus performance when writing data for reading back with fastparquet.\n",
    "\n",
    "[GitHub page](https://github.com/dask/fastparquet)\n",
    "\n",
    "[Docs](https://fastparquet.readthedocs.io/en/latest/)\n",
    "\n",
    "### Variations\n",
    "\n",
    "- uncompressed, gzip, snappy (install `python-snappy` from conda-forge separately)\n",
    "\n",
    "### Issues\n",
    "\n",
    "- As of 0.3.3, pandas extended dtypes like \"Int64\" are not supported, but there is [wip](https://github.com/dask/fastparquet/pull/483)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "import ig_format\n",
    "from ig_format import pandas as igpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './out/extracts/100k'\n",
    "schema_path = './out/schema.json'\n",
    "data_years = range(1997, 2000)\n",
    "\n",
    "dt = igpd.dtypes_from_schema(schema_path)\n",
    "df = pd.read_csv(data_dir + '/2000.csv', dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nullable ints to floats for fastparquet compatibility\n",
    "# could be part of schema creation\n",
    "for c in df:\n",
    "    if isinstance(df[c].dtype, pd.Int64Dtype):\n",
    "        df[c] = df[c].astype('float64')\n",
    "\n",
    "fp.write('./tmp/2000.parquet', df, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = fp.ParquetFile('./tmp/2000.parquet')\n",
    "dfp = pf.to_pandas(['company', 'abi', 'state', 'naics', 'employees'])\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Arrow\n",
    "\n",
    "> Apache Arrow is a cross-language development platform for **in-memory** data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication.\n",
    "\n",
    "[Website](https://arrow.apache.org/)\n",
    "[Docs](https://arrow.apache.org/docs/index.html)\n",
    "\n",
    "The Arrow Python bindings (also named “PyArrow”) have first-class integration with NumPy, pandas, and built-in Python objects. They are based on the C++ implementation of Arrow.\n",
    "\n",
    "[Data types, Schema and Table](https://arrow.apache.org/docs/python/data.html)\n",
    "\n",
    "[Read and write parquet](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "> fastparquet has a much smaller footprint, weighing in at a modest 180 kB compared to pyarrow‘s hulking 48 MB. It also has a much simpler and pandas centric read/write signature which can be nice for users that are more comfortable working with a DataFrame mindset. On the other hand,pyarrow offers greater coverage of the Parquet file format, more voluminous documentation, and more explicit column typing which turned out to be important later on. After much wavering we decided to go forward using pyarrow. [parquet business use case](https://medium.com/when-i-work-data/por-que-parquet-2a3ec42141c6)\n",
    "\n",
    "Arrow's purpose is to move data between components (pandas, parquet, CSV, Spark, R, ...) more efficiently. If we only use pandas, fastparquet might be sufficient. For now, Arrow does not support SAS or Stata.\n",
    "\n",
    "\n",
    "### Feather\n",
    "\n",
    "[docs](https://arrow.apache.org/docs/python/ipc.html#feather-format)\n",
    "\n",
    "Lightweight file storage format for dataframes understood by both pandas and R. Does not look very active, it is probably better to use parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# omnischema\n",
    "\n",
    "This is an idea of a Python package that would provide conversion between different table schema formats.\n",
    "\n",
    "What I need now: conversion between pandas and parquet.\n",
    "\n",
    "Schemas that could be supported:\n",
    "\n",
    "- pandas\n",
    "  - classic [numpy dtypes](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html)\n",
    "  - new [extension types](https://pandas.pydata.org/docs/getting_started/basics.html#basics-dtypes)\n",
    "- [parquet](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md)\n",
    "- [Frictionless Data Table Schema](https://frictionlessdata.io/specs/table-schema/)\n",
    "- SQL (flavors)\n",
    "- Apache Arrow\n",
    "- R dataframe\n",
    "- Stata\n",
    "- SAS\n",
    "- BigQuery\n",
    "- xlsx, ods\n",
    "\n",
    "## Existing tools\n",
    "\n",
    "- [Stat/Transfer](https://stattransfer.com/) - commercial software for format conversion. It even support Feather.\n",
    "- [pyreadstat](https://github.com/Roche/pyreadstat)\n",
    "  - convert SAS, Stata and SPSS formats <-> pandas dataframes\n",
    "  - wrapped C library, faster than sas7bdat and pd.read_sas\n",
    "  - available from conda-forge\n",
    "  - read meta, read column subset, read row chunks\n",
    "- [odo](http://odo.pydata.org/en/latest/) - efficiently migrates data from the source to the target through a network of conversions.\n",
    "\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "- [DCAT](https://www.w3.org/TR/vocab-dcat/) - W3C spec for dataset metadata\n",
    "- [Schema.org Dataset](https://schema.org/Dataset) - founded by major search engines, used by Google Dataset Search tool.\n",
    "- [Data Documentation Initiative](https://ddialliance.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
